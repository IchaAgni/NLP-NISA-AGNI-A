{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XHcvlTHli5T",
        "outputId": "c44592ab-b1c2-4237-cb4e-07913ea4f82b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import random\n",
        "import string\n",
        "import warnings\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download(\"popular\", quiet=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLYlnZ99ls4x",
        "outputId": "eb7ce058-f983-442a-dcf5-47beabb01c36"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "f = open('./drive/MyDrive/NLP2023/chatbot.txt', 'r')\n",
        "raw=f.read()\n",
        "raw = raw.lower()# converts to lowercase\n",
        "sent_tokens = nltk.sent_tokenize(raw)# converts to list of sentences\n",
        "word_tokens = nltk.word_tokenize(raw)# converts to list of words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XvEEQ3XnFP7",
        "outputId": "11dac71a-32f5-4161-9bfa-7ca8bfb801d5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "#WordNet is a semantically-oriented dictionary of English included in NLTK.\n",
        "def LemTokens(tokens):\n",
        "  return [lemmer.lemmatize(token) for token in tokens]\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "def LemNormalize(text):\n",
        "  return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ],
      "metadata": {
        "id": "Crvd0iVQprGW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GREETING_INPUTS = [\"hello\", \"hi\", \"greetings\", \"sup\", \"what’s up\",\"hey\",\"yo\"]\n",
        "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
        "def greeting(sentence):\n",
        "  for word in sentence.split():\n",
        "   if word.lower() in GREETING_INPUTS:\n",
        "    return random.choice(GREETING_RESPONSES)"
      ],
      "metadata": {
        "id": "sqbh100KpxB9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def response(user_response):\n",
        " spar_response=\"\"\n",
        " sent_tokens.append(user_response)\n",
        " TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words=\"english\")\n",
        " tfidf = TfidfVec.fit_transform(sent_tokens)\n",
        " vals = cosine_similarity(tfidf[-1], tfidf)\n",
        " idx=vals.argsort()[0][-2]\n",
        " flat = vals.flatten()\n",
        " flat.sort()\n",
        " req_tfidf = flat[-2]\n",
        " if(req_tfidf==0):\n",
        "  spar_response=spar_response+'I don’t understand you'\n",
        "  return spar_response\n",
        " else:\n",
        "  spar_response = spar_response+sent_tokens[idx]\n",
        "  return spar_response"
      ],
      "metadata": {
        "id": "8o1vEEuXqOfw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flag=True\n",
        "print(\"Spar: My name is Spar. I will answer your queries about Chatbots. If you want to exit, type Bye!\")\n",
        "while(flag==True):\n",
        " user_response = input()\n",
        " user_response=user_response.lower()\n",
        " if(user_response!='bye'):\n",
        "  if(user_response=='thanks' or user_response=='thank you' ):\n",
        "    flag=False\n",
        "    print('Spar: You are welcome..')\n",
        "  else:\n",
        "    if(greeting(user_response)!=None):\n",
        "      print('Spar: '+greeting(user_response))\n",
        "    else:\n",
        "     print('Spar: ',end='')\n",
        "     print(response(user_response))\n",
        "     sent_tokens.remove(user_response)\n",
        " else:\n",
        "  flag=False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iR_6TYyqQDM",
        "outputId": "692218a4-19c3-498f-c2f9-bb24b65411e5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spar: My name is Spar. I will answer your queries about Chatbots. If you want to exit, type Bye!\n",
            "im eating\n",
            "Spar: I don’t understand you\n",
            "im reading\n",
            "Spar: [5]\n",
            "\n",
            "\n",
            "contents\n",
            "1\tbackground\n",
            "2\tdevelopment\n",
            "3\tapplication\n",
            "3.1\tmessaging apps\n",
            "3.1.1\tas part of company apps and websites\n",
            "3.1.2\tchatbot sequences\n",
            "3.2\tcompany internal platforms\n",
            "3.3\tcustomer service\n",
            "3.4\thealthcare\n",
            "3.5\tpolitics\n",
            "3.6\ttoys\n",
            "3.7\tmalicious use\n",
            "4\tlimitations of chatbots\n",
            "5\tchatbots and jobs\n",
            "6\tsee also\n",
            "7\treferences\n",
            "7.1\tbibliography\n",
            "8\tfurther reading\n",
            "9\texternal links\n",
            "background\n",
            "in 1950, alan turing's famous article \"computing machinery and intelligence\" was published,[6] which proposed what is now called the turing test as a criterion of intelligence.\n",
            "lemon tea\n",
            "Spar: I don’t understand you\n",
            "hello\n",
            "Spar: I am glad! You are talking to me\n",
            "yes, how are you\n",
            "Spar: I don’t understand you\n",
            "yo busy?\n",
            "Spar: I am glad! You are talking to me\n",
            "bye\n"
          ]
        }
      ]
    }
  ]
}